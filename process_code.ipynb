{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dictionary : Turn it into a set\n",
    "vocab = set([line.rstrip() for line in open(\"document/vocab.txt\")])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_candidates(word):\n",
    "    # generate 1 word\n",
    "    # 1.insert 2.delete 3.replace\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    # 1.insert operation\n",
    "    inserts = [L+c+R for L,R in splits for c in letters]\n",
    "    # 2.delete operation\n",
    "    deletes = [L+R[1:] for L,R in splits if R]\n",
    "    # 3.replace operation\n",
    "    replaces = [L+c+R[1:] for L,R in splits if R for c in letters]\n",
    "\n",
    "    candidates = set(inserts+deletes+replaces)\n",
    "\n",
    "    return [word for word in candidates if word in vocab]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "# read the corpus\n",
    "categories = reuters.categories()\n",
    "corpus = reuters.sents(categories=categories)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\28224/nltk_data'\n    - 'D:\\\\softwareother\\\\Anaconda3\\\\envs\\\\tensorflow\\\\nltk_data'\n    - 'D:\\\\softwareother\\\\Anaconda3\\\\envs\\\\tensorflow\\\\share\\\\nltk_data'\n    - 'D:\\\\softwareother\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\28224\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-eb0b3fc6761b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mterm_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mbigram_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;31m# preprocess the first one\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'<s>'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mdoc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001B[0m in \u001B[0;36miterate_from\u001B[1;34m(self, start_tok)\u001B[0m\n\u001B[0;32m    422\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    423\u001B[0m             \u001B[1;31m# Get everything we can from this piece.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 424\u001B[1;33m             \u001B[1;32myield\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpiece\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miterate_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_tok\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0moffset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    425\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    426\u001B[0m             \u001B[1;31m# Update the offset table.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001B[0m in \u001B[0;36miterate_from\u001B[1;34m(self, start_tok)\u001B[0m\n\u001B[0;32m    304\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_current_toknum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtoknum\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    305\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_current_blocknum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mblock_index\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 306\u001B[1;33m             \u001B[0mtokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stream\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    307\u001B[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001B[0;32m    308\u001B[0m                 \u001B[1;34m\"block reader %s() should return list or tuple.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001B[0m in \u001B[0;36m_read_sent_block\u001B[1;34m(self, stream)\u001B[0m\n\u001B[0;32m    124\u001B[0m                 [\n\u001B[0;32m    125\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_word_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 126\u001B[1;33m                     \u001B[1;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sent_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpara\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    127\u001B[0m                 ]\n\u001B[0;32m    128\u001B[0m             )\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    901\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    902\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 903\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    904\u001B[0m         \u001B[1;31m# This looks circular, but its not, since __load() changes our\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    905\u001B[0m         \u001B[1;31m# __class__ to something new:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    893\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    894\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 895\u001B[1;33m         \u001B[0mresource\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    896\u001B[0m         \u001B[1;31m# This is where the magic happens!  Transform ourselves into\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    897\u001B[0m         \u001B[1;31m# the object by modifying our own __dict__ and __class__ to\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[0;32m    748\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    749\u001B[0m     \u001B[1;31m# Load the resource.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 750\u001B[1;33m     \u001B[0mopened_resource\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_url\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    751\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    752\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mformat\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"raw\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36m_open\u001B[1;34m(resource_url)\u001B[0m\n\u001B[0;32m    874\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    875\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mprotocol\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"nltk\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 876\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    877\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"file\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    878\u001B[0m         \u001B[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\softwareother\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m     \u001B[0msep\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"*\"\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m70\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 583\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    584\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\28224/nltk_data'\n    - 'D:\\\\softwareother\\\\Anaconda3\\\\envs\\\\tensorflow\\\\nltk_data'\n    - 'D:\\\\softwareother\\\\Anaconda3\\\\envs\\\\tensorflow\\\\share\\\\nltk_data'\n    - 'D:\\\\softwareother\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\28224\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# construct the language model : using bi-gram\n",
    "term_count = {}\n",
    "bigram_count = {}\n",
    "for doc in corpus:\n",
    "    # preprocess the first one\n",
    "    doc = ['<s>'] + doc\n",
    "    for i in range(0, len(doc)-1):\n",
    "        # bigram:[i,i+1]\n",
    "        term = doc[i]\n",
    "        bigram = doc[i:i+2]\n",
    "\n",
    "        if term in term_count:\n",
    "            term_count[term] += 1\n",
    "        else:\n",
    "            term_count[term] = 1\n",
    "\n",
    "        bigram = ' '.join(bigram)\n",
    "        if bigram in bigram_count:\n",
    "            bigram_count[bigram] += 1\n",
    "        else:\n",
    "            bigram_count[bigram] = 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# channel probability\n",
    "channel_prob = {}\n",
    "for line in open('document/spell-errors.txt'):\n",
    "    items = line.split(\":\")\n",
    "    correct = items[0].strip()\n",
    "    mistakes = [item.strip() for item in items[1].strip().split(\",\")]\n",
    "    channel_prob[correct] = {}\n",
    "    for mis in mistakes:\n",
    "        channel_prob[correct][mis] = 1.0/len(mistakes)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "V = len(term_count.keys())\n",
    "\n",
    "file = open('document/testdata.txt','r')\n",
    "for line in file:\n",
    "    items = line.rstrip().split('\\t')\n",
    "    line = items[2].split()\n",
    "    for word in line:\n",
    "        candidates = generate_candidates(word)\n",
    "        probs = []\n",
    "        # return the most possible one\n",
    "        for candi in candidates:\n",
    "            prob = 0\n",
    "            # a.channel\n",
    "            if candi in channel_prob and word in channel_prob[candi]:\n",
    "                prob += np.log(channel_prob[candi][word])\n",
    "            else:\n",
    "                prob += np.log(0.0001)\n",
    "\n",
    "            # calculate the possibility of language model:\n",
    "            idx = items[2].index(word) + 1\n",
    "            if items[2][idx - 1] in bigram_count and candi in bigram_count[items[2][idx-1]]:\n",
    "                prob += np.log((bigram_count[items[2][idx-1]][candi] + 1.0) / (term_count[bigram_count[items[2][idx-1]]] + V))\n",
    "            else:\n",
    "                prob += np.log(1.0 / V)\n",
    "\n",
    "        probs.append(prob)\n",
    "    max_idx = probs.index(max(probs))\n",
    "    print(word,candidates[max_idx])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tensorjupyter",
   "language": "python",
   "display_name": "Python[conda env:tensorjupyter]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}